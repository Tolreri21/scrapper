import requests
from bs4 import BeautifulSoup
import json
import time

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
# –ë–∞–∑–æ–≤—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –¥–æ–ø–æ–ª–Ω—è—Ç—å—Å—è –Ω–æ–º–µ—Ä–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
BASE_CATALOG_URL = 'https://www.rabota.md/ro/vacancies/category/it'
BASE_URL = 'https://www.rabota.md'
PAGES_TO_SCRAPE = 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–æ—Ç 1 –¥–æ 4)

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç –±—Ä–∞—É–∑–µ—Ä–∞
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

# 1. –®–ê–ì 1: –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –°–ë–û–† –°–°–´–õ–û–ö –°–û –í–°–ï–• –°–¢–†–ê–ù–ò–¶
# =========================================================

all_vacancy_urls = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
print(f"–ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å {PAGES_TO_SCRAPE} —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞—á–∏–Ω–∞—è —Å: {BASE_CATALOG_URL}")

# –¶–∏–∫–ª –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü (–æ—Ç 1 –¥–æ PAGES_TO_SCRAPE –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)
for page_num in range(1, PAGES_TO_SCRAPE + 1):

    # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (page_num=1) –Ω–µ –∏–º–µ–µ—Ç /1,
    # –ø–æ—ç—Ç–æ–º—É –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü 2 –∏ –¥–∞–ª–µ–µ.
    if page_num == 1:
        catalog_url = BASE_CATALOG_URL
    else:
        catalog_url = f"{BASE_CATALOG_URL}/{page_num}"

    print(f"\n---> –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}: {catalog_url}")

    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º GET-–∑–∞–ø—Ä–æ—Å
        response = requests.get(catalog_url, headers=headers)
        response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏ HTTP

        catalog_soup = BeautifulSoup(response.text, 'html.parser')

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ç–µ–≥–∏ <a>, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        # –°–µ–ª–µ–∫—Ç–æ—Ä: –∏—â–µ–º —Ç–µ–≥–∏ <a> —Å href, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º '/vacancies/'
        links_found_on_page = 0
        for link_tag in catalog_soup.select('a[href*="/vacancies/"]'):
            href = link_tag.get('href')

            # –§–∏–ª—å—Ç—Ä—É–µ–º, —á—Ç–æ–±—ã –±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            if '/vacancies/' in href and not href.endswith('vacancies'):
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π URL
                full_url = f"{BASE_URL}{href}" if href.startswith('/') else href
                all_vacancy_urls.add(full_url)
                links_found_on_page += 1

        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {links_found_on_page} —Å—Å—ã–ª–æ–∫. –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(all_vacancy_urls)}")

        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ –∫–∞—Ç–∞–ª–æ–≥—É
        time.sleep(1.5)

    except requests.exceptions.HTTPError as http_err:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {http_err}")
        # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ 50 –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç),
        # –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –ø—Ä–µ—Ä–≤–∞—Ç—å —Ü–∏–∫–ª, —Ç–∞–∫ –∫–∞–∫ –¥–∞–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–µ—Ç.
        if response.status_code == 404:
            print("   –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é, —á—Ç–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞. –ü—Ä–µ—Ä—ã–≤–∞—é —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫.")
            break
    except Exception as err:
        print(f"   ‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {err}")
        time.sleep(2)  # –î–∞–µ–º —á—É—Ç—å –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞, –ø—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å

# 2. –®–ê–ì 2: –°–∫—Ä–∞–ø–∏–Ω–≥ –∫–∞–∂–¥–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
# =========================================================

# –û–±—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
all_vacancies = []

if not all_vacancy_urls:
    print("\n‚ö†Ô∏è –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø—É—Å—Ç. –ù–µ—á–µ–≥–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å.")
else:
    print(f"\n--- –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(all_vacancy_urls)} —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π ---")

    for url in all_vacancy_urls:
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º GET-–∑–∞–ø—Ä–æ—Å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---
            title_tag = soup.find('h1', class_='vacancy-title')
            title = title_tag.get_text(strip=True) if title_tag else '–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'

            description_div = soup.find('div', class_='inbody')
            description = description_div.get_text('\n', strip=True) if description_div else '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            vacancy_data = {
                'url': url,
                'title': title,
                'description': description
            }

            all_vacancies.append(vacancy_data)

            # print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω URL: {url}")

            # !!! –í–∞–∂–Ω–æ: –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º –≤–∞–∫–∞–Ω—Å–∏—è–º
            time.sleep(1)

        except requests.exceptions.HTTPError as http_err:
            print(f"‚ùå –û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {http_err}")
        except Exception as err:
            print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {err}")

# 3. –®–ê–ì 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
# =========================================================

if all_vacancies:
    file_name = 'all_vacancies_paginated.json'
    with open(file_name, 'w', encoding='utf-8') as f:
        json.dump(all_vacancies, f, ensure_ascii=False, indent=4)

    print(f"\nüéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ ({len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π) —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {file_name}")
else:
    print("\n‚ö†Ô∏è –ù–µ –±—ã–ª–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")