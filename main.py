import requests
from bs4 import BeautifulSoup
import json
import time

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
# –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫—É—é –≤–µ—Ä—Å–∏—é –∫–∞—Ç–∞–ª–æ–≥–∞, —Å—É–¥—è –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É HTML-–∫–æ–¥—É
x = "backend"
BASE_CATALOG_URL = f'https://www.rabota.md/ro/jobs-moldova-{x}'
BASE_URL = 'https://www.rabota.md'
PAGES_TO_SCRAPE = 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–∏–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ 4 –∏–ª–∏ –±–æ–ª–µ–µ)

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

# 1. –®–ê–ì 1: –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –°–ë–û–† –°–°–´–õ–û–ö –°–û –í–°–ï–• –°–¢–†–ê–ù–ò–¶
# =========================================================

all_vacancy_urls = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
print(f"–ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å {PAGES_TO_SCRAPE} —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞—á–∏–Ω–∞—è —Å: {BASE_CATALOG_URL}")

for page_num in range(1, PAGES_TO_SCRAPE + 1):

    if page_num == 1:
        catalog_url = BASE_CATALOG_URL
    else:
        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–π –≤–µ—Ä—Å–∏–∏: /ru/vacancies/category/it/2
        catalog_url = f"{BASE_CATALOG_URL}/{page_num}"

    print(f"\n---> –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}: {catalog_url}")

    try:
        response = requests.get(catalog_url, headers=headers)
        response.raise_for_status()

        catalog_soup = BeautifulSoup(response.text, 'html.parser')

        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –§–ò–õ–¨–¢–† –°–°–´–õ–û–ö ---
        # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <a>, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –∫–ª–∞—Å—Å 'vacancyShowPopup'.
        links_found_on_page = 0
        for link_tag in catalog_soup.select('a.vacancyShowPopup'):
            href = link_tag.get('href')

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –õ–Æ–ë–û–ô –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            is_valid_vacancy_path = (
                    '/joburi/' in href or
                    '/vacancies/' in href or
                    '/locuri-de-munca/' in href  # <--- –î–û–ë–ê–í–õ–ï–ù–û –≠–¢–û –£–°–õ–û–í–ò–ï
            )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –¥–ª–∏–Ω—É –ø—É—Ç–∏, —á—Ç–æ–±—ã –æ—Ç—Å–µ—è—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
            if href and is_valid_vacancy_path and len(href.split('/')) > 4:
                full_url = f"{BASE_URL}{href}" if href.startswith('/') else href
                all_vacancy_urls.add(full_url)
                links_found_on_page += 1

        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {links_found_on_page} —Å—Å—ã–ª–æ–∫. –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(all_vacancy_urls)}")

        time.sleep(1.5)

    except requests.exceptions.HTTPError as http_err:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {http_err}")
        if response.status_code == 404:
            print("   –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é, —á—Ç–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞. –ü—Ä–µ—Ä—ã–≤–∞—é —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫.")
            break
    except Exception as err:
        print(f"   ‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {err}")
        time.sleep(2)

# 2. –®–ê–ì 2: –°–∫—Ä–∞–ø–∏–Ω–≥ –∫–∞–∂–¥–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
# =========================================================

all_vacancies = []

if not all_vacancy_urls:
    print("\n‚ö†Ô∏è –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø—É—Å—Ç. –ù–µ—á–µ–≥–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å.")
else:
    print(f"\n--- –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(all_vacancy_urls)} —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π ---")

    for i, url in enumerate(all_vacancy_urls):
        print(i)
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---
            title_tag = soup.find('h1', class_=['vacancy-title' , "text-3xl"])
            title = title_tag.get_text(strip=True) if title_tag else '–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'

            # --- –ì–ò–ë–ö–ò–ô –ü–û–ò–°–ö –û–ü–ò–°–ê–ù–ò–Ø ---
            description = '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'
            description_div = soup.find('div', class_= ['vacancy-content', 'inbody'])

            if description_div:
                description = description_div.get_text('\n', strip=True)

            vacancy_data = {
                'url': url,
                'title': title,
                'description': description,
                "category" : "Technical",
                "branch" : x
            }

            all_vacancies.append(vacancy_data)
            time.sleep(1)

        except requests.exceptions.HTTPError as http_err:
            print(f"‚ùå –û—à–∏–±–∫–∞ HTTP –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {http_err}")
        except Exception as err:
            print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {err}")

# 3. –®–ê–ì 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
# =========================================================

if all_vacancies:
    file_name = 'all_vacancies_final.json'
    with open(file_name, 'a', encoding='utf-8') as f:
        json.dump(all_vacancies, f, ensure_ascii=False, indent=4)

    print(f"\nüéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ ({len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π) —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {file_name}")
else:
    print("\n‚ö†Ô∏è –ù–µ –±—ã–ª–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")